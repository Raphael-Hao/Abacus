# multihead self-attention layer
# mb = 1, num_heads = 16, hidden_size = 1024, t_x = t_y = 40
mb16m40n40k64
mb16m40n64k40
# mb = 128, num_heads = 16, hidden_size = 1024, t_x = t_y = 40
mb2048m40n40k64
mb2048m40n64k40
